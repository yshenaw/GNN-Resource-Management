{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f05f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio                     \n",
    "import numpy as np                         \n",
    "import matplotlib.pyplot as plt \n",
    "import os  \n",
    "os.chdir('/home/yifeishen/GNN-Resource-Management/D2D/Weighted Power Control')        \n",
    "import function_wmmse_powercontrol as wf\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN\n",
    "import wireless_networks_generator as wg\n",
    "from FPLinQ import FP_optimize, FP\n",
    "import helper_functions\n",
    "import time\n",
    "\n",
    "class init_parameters():\n",
    "    def __init__(self):\n",
    "        # wireless network settings\n",
    "        self.n_links = train_K\n",
    "        self.field_length = 1000\n",
    "        self.shortest_directLink_length = 2\n",
    "        self.longest_directLink_length = 65\n",
    "        self.shortest_crossLink_length = 1\n",
    "        self.bandwidth = 5e6\n",
    "        self.carrier_f = 2.4e9\n",
    "        self.tx_height = 1.5\n",
    "        self.rx_height = 1.5\n",
    "        self.antenna_gain_decibel = 2.5\n",
    "        self.tx_power_milli_decibel = 40\n",
    "        self.tx_power = np.power(10, (self.tx_power_milli_decibel-30)/10)\n",
    "        self.noise_density_milli_decibel = -169\n",
    "        self.input_noise_power = np.power(10, ((self.noise_density_milli_decibel-30)/10)) * self.bandwidth\n",
    "        self.output_noise_power = self.input_noise_power\n",
    "        self.SNR_gap_dB = 6\n",
    "        self.SNR_gap = np.power(10, self.SNR_gap_dB/10)\n",
    "        self.setting_str = \"{}_links_{}X{}_{}_{}_length\".format(self.n_links, self.field_length, self.field_length, self.shortest_directLink_length, self.longest_directLink_length)\n",
    "        # 2D occupancy grid setting\n",
    "        self.cell_length = 5\n",
    "        self.n_grids = np.round(self.field_length/self.cell_length).astype(int)\n",
    "\n",
    "def proc_train_losses(train_path_losses, train_channel_losses):\n",
    "    mask = np.eye(train_K)\n",
    "    diag_path = np.multiply(mask,train_path_losses)\n",
    "    off_diag_path = train_path_losses - diag_path\n",
    "    diag_channel = np.multiply(mask,train_channel_losses)\n",
    "    train_losses = diag_channel + off_diag_path\n",
    "    return train_losses\n",
    "\n",
    "def normalize_data(train_data,test_data):\n",
    "    mask = np.eye(train_K)\n",
    "    train_copy = np.copy(train_data)\n",
    "    diag_H = np.multiply(mask,train_copy)\n",
    "    diag_mean = np.sum(diag_H)/train_layouts/train_K\n",
    "    diag_var = np.sqrt(np.sum(np.square(diag_H))/train_layouts/train_K)\n",
    "    tmp_diag = (diag_H - diag_mean)/diag_var\n",
    "\n",
    "    off_diag = train_copy - diag_H\n",
    "    off_diag_mean = np.sum(off_diag)/train_layouts/train_K/(train_K-1)\n",
    "    off_diag_var = np.sqrt(np.sum(np.square(off_diag))/train_layouts/train_K/(train_K-1))\n",
    "    tmp_off = (off_diag - off_diag_mean)/off_diag_var\n",
    "    tmp_off_diag = tmp_off - np.multiply(tmp_off,mask)\n",
    "    \n",
    "    norm_train = np.multiply(tmp_diag,mask) + tmp_off_diag\n",
    "    \n",
    "    # normlize test\n",
    "    mask = np.eye(test_K)\n",
    "    test_copy = np.copy(test_data)\n",
    "    diag_H = np.multiply(mask,test_copy)\n",
    "    tmp_diag = (diag_H - diag_mean)/diag_var\n",
    "    \n",
    "    off_diag = test_copy - diag_H\n",
    "    tmp_off = (off_diag - off_diag_mean)/off_diag_var\n",
    "    tmp_off_diag = tmp_off - np.multiply(tmp_off,mask)\n",
    "    \n",
    "    norm_test = np.multiply(tmp_diag,mask) + tmp_off_diag\n",
    "    \n",
    "    return norm_train, norm_test\n",
    "\n",
    "def simple_greedy(X,AAA,label):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    thd = int(np.sum(label)/n)\n",
    "    Y = np.zeros((n,test_K))\n",
    "    for ii in range(n):\n",
    "        alpha = AAA[ii,:]\n",
    "        H_diag = alpha * np.square(np.diag(X[ii,:,:]))\n",
    "        xx = np.argsort(H_diag)[::-1]\n",
    "        for jj in range(thd):\n",
    "            Y[ii,xx[jj]] = 1\n",
    "    return Y\n",
    "\n",
    "def build_graph(loss, dist, norm_dist, norm_loss, K, threshold):\n",
    "    n = loss.shape[0]\n",
    "    x1 = np.expand_dims(np.diag(norm_dist),axis=1)\n",
    "    x2 = np.expand_dims(np.diag(norm_loss),axis=1)\n",
    "    x3 = np.ones((K,1))\n",
    "    x = np.concatenate((x1,x2,x3),axis=1)\n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    \n",
    "    dist2 = np.copy(dist)\n",
    "    mask = np.eye(K)\n",
    "    diag_dist = np.multiply(mask,dist2)\n",
    "    dist2 = dist2 + 1000* diag_dist\n",
    "    dist2[dist2 > threshold] = 0\n",
    "    attr_ind = np.nonzero(dist2)\n",
    "    e1 = np.expand_dims(norm_loss[attr_ind],  axis = -1)\n",
    "    e2 = np.expand_dims((norm_loss.transpose(0,1))[attr_ind],  axis = -1)\n",
    "    e3 = np.expand_dims(norm_dist[attr_ind],  axis = -1)\n",
    "    edge_attr = np.concatenate((e1,e2,e3),axis=-1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    attr_ind = np.array(attr_ind)\n",
    "    adj = np.zeros(attr_ind.shape)\n",
    "    adj[0,:] = attr_ind[1,:]\n",
    "    adj[1,:] = attr_ind[0,:]\n",
    "    edge_index = torch.tensor(adj, dtype=torch.long)\n",
    "\n",
    "    y = torch.tensor(np.expand_dims(loss,axis=0), dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index.contiguous(),edge_attr = edge_attr, y = y)\n",
    "    return data\n",
    "\n",
    "def proc_data(HH, dists, norm_dists, norm_HH, K):\n",
    "    n = HH.shape[0]\n",
    "    data_list = []\n",
    "    for i in range(n):\n",
    "        data = build_graph(HH[i,:,:],dists[i,:,:], norm_dists[i,:,:], norm_HH[i,:,:], K,300)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "class IGConv(MessagePassing):\n",
    "    def __init__(self, mlp1, mlp2, **kwargs):\n",
    "        super(IGConv, self).__init__(aggr='max', **kwargs)\n",
    "\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        #self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.mlp1)\n",
    "        reset(self.mlp2)\n",
    "        \n",
    "    def update(self, aggr_out, x):\n",
    "        tmp = torch.cat([x, aggr_out], dim=1)\n",
    "        comb = self.mlp2(tmp)\n",
    "        return torch.cat([x[:,:2], comb],dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        edge_attr = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=1)\n",
    "        agg = self.mlp1(tmp)\n",
    "        return agg\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.mlp1,self.mlp2)\n",
    "\n",
    "def MLP(channels, batch_norm=True):\n",
    "    return Seq(*[\n",
    "        Seq(Lin(channels[i - 1], channels[i]), ReLU())#, BN(channels[i]))\n",
    "        for i in range(1, len(channels))\n",
    "    ])\n",
    "class IGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IGCNet, self).__init__()\n",
    "\n",
    "        self.mlp1 = MLP([6, 32, 32])\n",
    "        self.mlp2 = MLP([35, 16])\n",
    "        self.mlp2 = Seq(*[self.mlp2,Seq(Lin(16, 1), Sigmoid())])\n",
    "        self.conv = IGConv(self.mlp1,self.mlp2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x0, edge_attr, edge_index = data.x, data.edge_attr, data.edge_index\n",
    "        x1 = self.conv(x = x0, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        x2 = self.conv(x = x1, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x3 = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        #x4 = self.conv(x = x3, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        out = self.conv(x = x2, edge_index = edge_index, edge_attr = edge_attr)\n",
    "        return out\n",
    "        \n",
    "\n",
    "def sr_loss(data, out, K):\n",
    "    power = out[:,2]\n",
    "    power = torch.reshape(power, (-1, K, 1)) \n",
    "    \n",
    "    abs_H_2 = data.y#torch.pow(data.y, 2)\n",
    "    abs_H_2 = abs_H_2.permute(0,2,1)\n",
    "    rx_power = torch.mul(abs_H_2, power)\n",
    "    mask = torch.eye(K)\n",
    "    mask = mask.to(device)\n",
    "    valid_rx_power = torch.sum(torch.mul(rx_power, mask), 1)\n",
    "    interference = torch.sum(torch.mul(rx_power, 1 - mask), 1) + var\n",
    "    rate = torch.log2(1 + torch.div(valid_rx_power, interference))\n",
    "    #w_rate = torch.mul(data.pos,rate)\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    loss = torch.neg(sum_rate)\n",
    "    return loss\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = sr_loss(data,out,train_K)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / train_layouts\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            out = model(data)\n",
    "            end = time.time()\n",
    "            print('dnn time',end-start)\n",
    "            loss = sr_loss(data,out,test_K)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "    power = out[:,2]\n",
    "    power = torch.reshape(power, (-1, test_K)) \n",
    "    Y = power.cpu().numpy()\n",
    "    rates = helper_functions.compute_rates(test_config, \n",
    "            Y, directLink_channel_losses, crossLink_channel_losses)\n",
    "    sr = np.mean(np.sum(rates,axis=1))\n",
    "    print('actual_rates:',sr)\n",
    "    \n",
    "    return total_loss / test_layouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f991d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<20000 layouts: 50_links_1000X1000_2_65_length>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<<<<<500 layouts: 50_links_1000X1000_2_65_length>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "train_K = 50\n",
    "test_K = 50\n",
    "train_layouts = 20000\n",
    "test_layouts = 500\n",
    "\n",
    "train_config = init_parameters()\n",
    "var = train_config.output_noise_power / train_config.tx_power\n",
    "layouts, train_dists = wg.generate_layouts(train_config, train_layouts)\n",
    "train_path_losses = wg.compute_path_losses(train_config, train_dists)\n",
    "train_channel_losses = helper_functions.add_shadowing(train_path_losses)\n",
    "train_channel_losses = helper_functions.add_fast_fading(train_channel_losses)\n",
    "train_losses = proc_train_losses(train_path_losses, train_channel_losses)\n",
    "\n",
    "test_config = init_parameters()\n",
    "layouts, test_dists = wg.generate_layouts(test_config, test_layouts)\n",
    "test_path_losses = wg.compute_path_losses(test_config, test_dists)\n",
    "test_channel_losses = helper_functions.add_shadowing(test_path_losses)\n",
    "test_channel_losses = helper_functions.add_fast_fading(test_channel_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ecfb2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_WMMSE_GPU(p_int, alpha, H, Pmax, var_noise):\n",
    "    # Auto-convert inputs and move to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Convert inputs to tensors and remove singleton dimensions\n",
    "    p_int = torch.as_tensor(p_int, dtype=torch.float32).to(device)\n",
    "    alpha = torch.as_tensor(alpha, dtype=torch.float32).to(device)\n",
    "    H = torch.as_tensor(H, dtype=torch.float32).to(device)\n",
    "    Pmax = torch.as_tensor(Pmax, dtype=torch.float32).to(device)\n",
    "    var_noise = torch.as_tensor(var_noise, dtype=torch.float32).to(device)\n",
    "\n",
    "    N, K, _ = p_int.shape\n",
    "    b = torch.sqrt(p_int)\n",
    "    mask = torch.eye(K, device=device)\n",
    "    \n",
    "    # Initialize tensors with proper dimensions\n",
    "    f = torch.zeros((N, K, 1), device=device)\n",
    "    w = torch.zeros((N, K, 1), device=device)\n",
    "    \n",
    "    # Initial calculations\n",
    "    rx_power = H * b  # (N,K,K)\n",
    "    valid_rx_power = (rx_power * mask).sum(dim=1)  # (N,K)\n",
    "    interference = rx_power.square().sum(dim=2) + var_noise\n",
    "    f = (valid_rx_power / interference)\n",
    "    w = 1 / (1 - f * valid_rx_power)\n",
    "    \n",
    "    for _ in range(100):\n",
    "        fp = f.unsqueeze(1) \n",
    "        H_T = H.permute(0, 2, 1) \n",
    "        rx_power = H_T * fp  \n",
    "        \n",
    "        valid_rx_power = (rx_power * mask).sum(1).squeeze(-1)\n",
    "        bup = alpha * w * valid_rx_power\n",
    "        \n",
    "        wp = w.unsqueeze(1)  \n",
    "        rx_power_sq = rx_power.square()\n",
    "        bdown = (alpha.unsqueeze(1) * rx_power_sq * wp).sum(2)\n",
    "        \n",
    "        b = torch.clamp(bup / (bdown + 1e-10), 0, torch.sqrt(Pmax))\n",
    "        \n",
    "\n",
    "        rx_power = H * b.unsqueeze(1)\n",
    "        valid_rx_power = (rx_power * mask).sum(1)\n",
    "        interference = rx_power.square().sum(2) + var_noise\n",
    "        f = (valid_rx_power / interference)\n",
    "        w = 1 / (1 - f * valid_rx_power)\n",
    "\n",
    "    return b.square().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "913df9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPlinQ fade: 199.6456248420678\n",
      "WMMSE fade: 200.16008775884217 time 0.9996109008789062\n",
      "WMMSE_GPU fade: 200.1567671220542 time 0.028956890106201172\n",
      "baseline: 159.62223337615055\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "norm_train_dists, norm_test_dists = normalize_data(1/train_dists,1/test_dists)\n",
    "norm_train_losses, norm_test_losses = normalize_data(np.sqrt(train_channel_losses),np.sqrt(test_channel_losses) )\n",
    "train_data_list = proc_data(train_channel_losses, train_dists, norm_train_dists, norm_train_losses, train_K)\n",
    "test_data_list = proc_data(test_channel_losses, test_dists, norm_test_dists, norm_test_losses, test_K)\n",
    "\n",
    "\n",
    "directLink_channel_losses = helper_functions.get_directLink_channel_losses(test_channel_losses)\n",
    "crossLink_channel_losses = helper_functions.get_crossLink_channel_losses(test_channel_losses)\n",
    "\n",
    "Pini = np.random.rand(test_layouts,test_K,1 )\n",
    "start_time = time.time()\n",
    "Y3 = batch_WMMSE_GPU(Pini,np.ones([test_layouts, test_K]),np.sqrt(test_channel_losses),1,var)\n",
    "end_time = time.time()\n",
    "time2 = end_time - start_time\n",
    "\n",
    "Y1 = FP_optimize(test_config,test_channel_losses,np.ones([test_layouts, test_K]))\n",
    "start_time = time.time()\n",
    "Y2 = wf.batch_WMMSE2(Pini,np.ones([test_layouts, test_K]),np.sqrt(test_channel_losses),1,var)\n",
    "end_time = time.time()\n",
    "time1 = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rates1 = helper_functions.compute_rates(test_config, \n",
    "            Y1, directLink_channel_losses, crossLink_channel_losses)\n",
    "rates2 = helper_functions.compute_rates(test_config, \n",
    "            Y2, directLink_channel_losses, crossLink_channel_losses)\n",
    "rates3 = helper_functions.compute_rates(test_config, \n",
    "            Y3, directLink_channel_losses, crossLink_channel_losses)\n",
    "\n",
    "\n",
    "sr1 = np.mean(np.sum(rates1,axis=1))\n",
    "sr2 = np.mean(np.sum(rates2,axis=1))\n",
    "sr3 = np.mean(np.sum(rates3,axis=1))\n",
    "print('FPlinQ fade:',sr1)\n",
    "print('WMMSE fade:',sr2, 'time', time1)\n",
    "print('WMMSE_GPU fade:',sr3, 'time', time2)\n",
    "\n",
    "bl_Y = simple_greedy(test_channel_losses,np.ones([test_layouts, test_K]),Y1)\n",
    "\n",
    "rates_bl = helper_functions.compute_rates(test_config, \n",
    "            bl_Y, directLink_channel_losses, crossLink_channel_losses)\n",
    "sr_bl = np.mean(np.sum(rates_bl,axis=1))\n",
    "print('baseline:',sr_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89437ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn time 0.003275632858276367\n",
      "actual_rates: 190.94908855569543\n",
      "Epoch 050, Train Loss: -191.6898, Val Loss: -190.9491\n",
      "dnn time 0.0033636093139648438\n",
      "actual_rates: 191.96558819182297\n",
      "Epoch 100, Train Loss: -192.4063, Val Loss: -191.9656\n",
      "dnn time 0.003934621810913086\n",
      "actual_rates: 192.0361204736302\n",
      "Epoch 150, Train Loss: -192.6345, Val Loss: -192.0361\n",
      "dnn time 0.003245830535888672\n",
      "actual_rates: 192.29132791042463\n",
      "Epoch 200, Train Loss: -192.8351, Val Loss: -192.2913\n",
      "dnn time 0.003136873245239258\n",
      "actual_rates: 192.25162173938332\n",
      "Epoch 250, Train Loss: -192.9529, Val Loss: -192.2516\n",
      "dnn time 0.003259420394897461\n",
      "actual_rates: 192.17358953595198\n",
      "Epoch 300, Train Loss: -192.7643, Val Loss: -192.1736\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = IGCNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "train_loader = DataLoader(train_data_list, batch_size=100, shuffle=True,num_workers=1)\n",
    "test_loader = DataLoader(test_data_list, batch_size=test_layouts, shuffle=False, num_workers=1)\n",
    "for epoch in range(1, 500):\n",
    "    loss1 = train()\n",
    "    if(epoch % 50 == 0):\n",
    "        loss2 = test()\n",
    "        print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "            epoch, loss1, loss2))\n",
    "    #scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "gnn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
